{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as psk\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyinputplus as pyin\n",
    "import warnings\n",
    "import mysql.connector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas.io.sql as psql\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/01 23:22:32 WARN Utils: Your hostname, Rajarshis-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.1.159 instead (on interface en0)\n",
      "23/03/01 23:22:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/01 23:22:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bank Transactions\") \\\n",
    "    .config(\"spark.jars\",\"/Users/roy/Downloads/mysql-connector-j-8.0.32/mysql-connector-j-8.0.32.jar\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m(SELECT cc.*, cust.cust_zip \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m        FROM cdw_sapp_credit_card as cc \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m        JOIN cdw_sapp_customer as cust ON cc.CUST_SSN = cust.SSN) as i\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m# define MySQL connection parameters\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mjdbc\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mjdbc:mysql://localhost:3306/creditcard_capstone\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdbtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, query) \\\n\u001b[1;32m     10\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mroot\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mpassword\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mShaShi3493*\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m---> 12\u001b[0m     \u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     14\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mjdbc\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjdbc:mysql://localhost:3306/creditcard_capstone\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mdbtable\u001b[39m\u001b[39m\"\u001b[39m, query) \\\n\u001b[1;32m     17\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mpassword\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mShaShi3493*\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m     \u001b[39m.\u001b[39mload()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:184\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\n"
     ]
    }
   ],
   "source": [
    "query = \"(SELECT cc.*, cust.cust_zip \\\n",
    "        FROM cdw_sapp_credit_card as cc \\\n",
    "        JOIN cdw_sapp_customer as cust ON cc.CUST_SSN = cust.SSN) as i\"\n",
    "\n",
    "\n",
    "# define MySQL connection parameters\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/creditcard_capstone\") \\\n",
    "    .option(\"dbtable\", query) \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"ShaShi3493*\") \\\n",
    "    .load()\n",
    "\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/creditcard_capstone\") \\\n",
    "    .option(\"dbtable\", query) \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"ShaShi3493*\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"(SELECT * FROM cdw_sapp_branch bc \\\n",
    "      JOIN cdw_sapp_credit_card cc ON bc.BRANCH_CODE = cc.BRANCH_CODE \\\n",
    "        JOIN cdw_sapp_customer as cust ON cc.CUST_SSN = cust.SSN)\"\n",
    "\n",
    "# define a function to execute SQL queries and return results\n",
    "def execute_query(query):\n",
    "    return spark.sql(query).toPandas()\n",
    "print(execute_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = \"(SELECT cc.*, cust.cust_zip \\\n",
    "        FROM cdw_sapp_credit_card as cc \\\n",
    "        JOIN cdw_sapp_customer as cust ON cc.CUST_SSN = cust.SSN) as a\"\n",
    "df_sp_cc_cust = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/creditcard_capstone\") \\\n",
    "    .option(\"dbtable\", query) \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"ShaShi3493*\") \\\n",
    "    .load()\n",
    "\n",
    "print(df)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # convert the Spark DataFrame to a Pandas DataFrame\n",
    "pd_df = df.toPandas()\n",
    "\n",
    "# display the first 5 rows of the Pandas DataFrame\n",
    "print(pd_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define a function to execute SQL queries and return results\n",
    "def execute_query(query, mysql_hostname, mysql_port, mysql_database, mysql_username, mysql_password):\n",
    "    url = f\"jdbc:mysql://{mysql_hostname}:{mysql_port}/{mysql_database}\"\n",
    "    properties = {\n",
    "        \"driver\": \"com.mysql.jdbc.Driver\",\n",
    "        \"user\": mysql_username,\n",
    "        \"password\": mysql_password\n",
    "    }\n",
    "    df = spark.read.jdbc(url=url, table=f\"({query}) as t\", properties=properties)\n",
    "    return df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# establish database connection\n",
    "cnx = mysql.connector.connect(user='root', password='ShaShi3493*',\n",
    "                              host='localhost', database='creditcard_capstone')\n",
    "\n",
    "# execute SQL query and store results in dataframe\n",
    "df = pd.read_sql(\"(SELECT * FROM cdw_sapp_branch bc JOIN cdw_sapp_credit_card cc ON bc.BRANCH_CODE = cc.BRANCH_CODE JOIN cdw_sapp_customer as cust ON cc.CUST_SSN = cust.SSN)\", con=cnx)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to execute SQL queries and return a PySpark DataFrame\n",
    "\n",
    "query = \"(SELECT * FROM cdw_sapp_branch bc JOIN cdw_sapp_credit_card cc ON bc.BRANCH_CODE = cc.BRANCH_CODE \\\n",
    "        JOIN cdw_sapp_customer as cust ON cc.CUST_SSN = cust.SSN)\"\n",
    "\n",
    "\n",
    "def execute_query(query):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", f\"jdbc:mysql://{mysql_hostname}:{mysql_port}/{mysql_database}\")\\\n",
    "            .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "                .option(\"user\", mysql_username)\\\n",
    "                    .option(\"password\", mysql_password)\\\n",
    "                        .option(\"dbtable\", \"({})\".format(query))\\\n",
    "                            .load()\n",
    "    return df           \n",
    "    # convert the Spark DataFrame to a Pandas DataFrame\n",
    "    pd_df = df.toPandas()\n",
    "\n",
    "# display the first 5 rows of the Pandas DataFrame\n",
    "print(pd_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)    Used to display the transactions made by customers living in a given zip code for a given month and year. \n",
    "def execute_query():\n",
    "\n",
    "    flag = True\n",
    "    zip_code = input(\"Enter Your Zip Code:  \")\n",
    "    while (flag):\n",
    "        if not zip_code.isdigit():\n",
    "            zip_code = input('enter a valid zipcode')\n",
    "        else:\n",
    "            flag=False\n",
    "    \n",
    "    transaction_year = input(\"Enter the year for transactions records:  \")\n",
    "    flag=True\n",
    "    while (flag):\n",
    "        if not transaction_year.isdigit() or len(transaction_year)!=4:\n",
    "            transaction_year = input(\"Enter valid year format XXXX\")\n",
    "        else:\n",
    "            flag = False\n",
    "        \n",
    "    transaction_month = input(\"Enter the month in the form (XX): \").zfill(2)\n",
    "    flag = True\n",
    "    while (flag):\n",
    "        if  not transaction_month.isdigit():\n",
    "            transaction_month = input('Please enter valid month')\n",
    "        else:\n",
    "            flag=False\n",
    "    \n",
    "\n",
    "    cursor.execute(query,(zip_code,transaction_year,transaction_month,))\n",
    "    records = cursor.fetchall()\n",
    "    print(\"\\n List of  customer transactions in a given Zip code : \")\n",
    "    print(\"\\n\\n Customer no\\t\\tdate\\t   cust_ssn  code  type       value  id\")\n",
    "    for row in records:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to validate user input\n",
    "\n",
    "def validate_input(prompt, expected_type):\n",
    "    while True:\n",
    "        user_input = pyin.inputStr(prompt)\n",
    "        try:\n",
    "            validated_input = expected_type(user_input)\n",
    "            return validated_input\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a {}.\".format(expected_type.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"Select an option:\")\n",
    "    print(\"1. Display transactions by zip code and month/year\")\n",
    "    print(\"2. Display number and total value of transactions by type\")\n",
    "    print(\"3. Display total number and value of transactions by state\")\n",
    "    print(\"4. Exit\")\n",
    "\n",
    "    # validate user input\n",
    "    choice = validate_input(\"Enter your choice: \", int)\n",
    "    if choice == 1:\n",
    "        #1)    Used to display the transactions made by customers living in a given zip code for a given month and year. Order by day in descending order.\n",
    "       \n",
    "        zip_code = validate_input(\"Enter the zip code: \", int)\n",
    "        year = validate_input(\"Enter the year (YYYY): \", int)\n",
    "        month = validate_input(\"Enter the month (MM): \", int)\n",
    "        result = execute_query(\"SELECT * FROM CDW_SAPP_CUSTOMER, CDW_SAPP_BRANCH WHERE CUST_ZIP = {} AND YEAR(date) = {} AND MONTH(date) = {} ORDER BY date DESC\".format(zip_code, year, month))\n",
    "        if result:\n",
    "            pd_result = result.toPandas()\n",
    "            print(pd_result)\n",
    "        else:\n",
    "            print(\"No transactions found.\")\n",
    "    elif choice == 2:\n",
    "        #2)    Used to display the number and total values of transactions for a given type.\n",
    "      \n",
    "        transaction_type = validate_input(\"Enter the transaction type: \", str)\n",
    "        result = execute_query(\"SELECT TRANSACTION_VALUE, TRANSACTION_TYPE FROM '{}' where TRANSACTION_TYPE  = '{}'\".format(query, transaction_type))\n",
    "        if result:\n",
    "            pd_result = result.toPandas()\n",
    "            print(pd_result)\n",
    "        else:\n",
    "            print(\"No transactions found.\")\n",
    "    elif choice == 3:\n",
    "        #3)    Used to display the total number and total values of transactions for branches in a given state.\n",
    "    \n",
    "        state = validate_input(\"Enter the state: \", str)\n",
    "        result = execute_query(\"SELECT bc.state, COUNT(*) as count, SUM(TRANSACTION_VALUE) as total_value FROM cdw_sapp_branch bc JOIN cdw_sapp_credit_card cc ON bc.BRANCH_CODE = cc.BRANCH_CODE WHERE BRANCH_STATE = '{}'\".format(state))\n",
    "        if result:\n",
    "            pd_result = result.toPandas()\n",
    "            print(pd_result)\n",
    "        else:\n",
    "            print(\"No transactions found.\")\n",
    "    elif choice == 4:\n",
    "        # exit\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
